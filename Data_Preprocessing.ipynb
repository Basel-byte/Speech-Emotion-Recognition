{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Basel-byte/Speech-Emotion-Recognition/blob/master/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N4XT3MU6exg"
      },
      "source": [
        "###**Importing Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTN8UKJK8RF2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Mounting Drive**"
      ],
      "metadata": {
        "id": "qqYhSpV-Ux_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glMs-H3Ytwqp",
        "outputId": "2652790d-3836-48e9-c89e-8bb178e27dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Extracting Audio Files**"
      ],
      "metadata": {
        "id": "DQM5JhmYU34K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y3Mro927-NE"
      },
      "outputs": [],
      "source": [
        "dir_path = \"/content/drive/MyDrive/Crema\"\n",
        "if not os.path.exists(dir_path):\n",
        "  !unzip /content/drive/MyDrive/Crema.zip -d /content/drive/MyDrive/Crema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McNWBh8l68VG"
      },
      "source": [
        "###**Reading Audio Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfG9F5ss-Hm-"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "audio_waves = []\n",
        "for file_name in os.listdir(dir_path):\n",
        "  labels.append(file_name[9:12])\n",
        "  audio_waves.append(librosa.load(os.path.join(dir_path, file_name)))\n",
        "lb = LabelBinarizer()\n",
        "encoded_labels = lb.fit_transform(labels)\n",
        "print(encoded_labels.shape)\n",
        "lb.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Audio signal augmentation**"
      ],
      "metadata": {
        "id": "yqEcUOnNU_d5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wdwVHWLxlZ3R"
      },
      "outputs": [],
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n",
        "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5) * 1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate=22050, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Extraction of frequency and time-domain features**"
      ],
      "metadata": {
        "id": "3DBGrU1oVJ88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MHeZhlQXmJhI"
      },
      "outputs": [],
      "source": [
        "def extract_features(signal): \n",
        "  mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)\n",
        "  sample = np.vstack((mfccs, librosa.feature.zero_crossing_rate(y=signal)))\n",
        "  sample = np.vstack((sample, librosa.feature.rms(y=signal)))\n",
        "  if mfccs.shape[1] < max_frames:\n",
        "    sample = np.pad(sample, ((0, 0), (0, max_frames - mfccs.shape[1])), mode='constant')\n",
        "  else:\n",
        "    sample = sample[:, :max_frames]\n",
        "  sampel = sample.reshape((42 * max_frames, 1))\n",
        "  return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3cwlx2C4wUgB"
      },
      "outputs": [],
      "source": [
        "def get_augmented_features(signal):\n",
        "  augmented_data = np.vstack(noise(signal), shift(signal))\n",
        "  extract_fn = np.vectoriz(extract_features)\n",
        "  augmented_data = extract_fn(augmented_data)\n",
        "  return augmented_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Preparing feature space for model 1**"
      ],
      "metadata": {
        "id": "OMkbvBbMWciL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LhjbHwi-zf-r"
      },
      "outputs": [],
      "source": [
        "max_frames = 200\n",
        "n_mfcc = 40\n",
        "n_features = n_mfcc + 2\n",
        "data = np.empty((len(labels), n_features * max_frames, 1))\n",
        "for i, audio in enumerate(audio_waves):\n",
        "  signal, sr = audio\n",
        "  data[i] = extract_features(signal)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data split 70% training(5% validation), 30% testing**"
      ],
      "metadata": {
        "id": "Eas7UMmHVsls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WDtuYTx9VB60"
      },
      "outputs": [],
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(data, encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=42)\n",
        "train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, train_labels, test_size=0.05, stratify=train_labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Circular Padding of Spectogram**"
      ],
      "metadata": {
        "id": "J-ClzZW9V9DE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OvZzXogOWQ7M"
      },
      "outputs": [],
      "source": [
        "def get_spectogram_circulary_padded(mel_spec, max_frames):\n",
        "  n_frames = mel_spec.shape[1]\n",
        "  if n_frames < max_frames:\n",
        "    n_pad = max_frames - n_frames\n",
        "    n = n_pad // n_frames\n",
        "    mod = n_pad % n_frames\n",
        "    pad = mel_spec[:, :mod]\n",
        "    result = np.hstack((mel_spec, pad))\n",
        "    for i in range(n):\n",
        "      result = np.hstack((result, mel_spec))\n",
        "    return result\n",
        "  else:\n",
        "    return mel_spec[:, :max_frames]\n",
        "\n",
        "# get_spectogram_circulary_padded(librosa.feature.melspectrogram(y=audio_waves[20][0]), max_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Preparing feature space for model 2**"
      ],
      "metadata": {
        "id": "mowYEn67WT-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iNIaAr-uYC-N"
      },
      "outputs": [],
      "source": [
        "data2 = np.empty((len(labels), 128, max_frames, 1))\n",
        "for i, audio in enumerate(audio_waves):\n",
        "  signal, sr = audio\n",
        "  mel_spec = librosa.feature.melspectrogram(y=signal)\n",
        "  mel_spec = get_spectogram_circulary_padded(mel_spec, max_frames)\n",
        "  # delta1 = librosa.feature.delta(mel_spec, delta=1)\n",
        "  # delta2 = librosa.feature.delta(mel_spec, delta=2)\n",
        "  # data2[i] = np.dstack((mel_spec, delta1, delta2)) \n",
        "  data2[i] = mel_spec.reshape((128, max_frames, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHNt6nK1hvZ1"
      },
      "outputs": [],
      "source": [
        "train_data2, test_data2, train_labels2, test_labels2 = train_test_split(data2, encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=42)\n",
        "train_data2, valid_data2, train_labels2, valid_labels2 = train_test_split(train_data2, train_labels2, test_size=0.05, stratify=train_labels2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH4C04qh4cp1"
      },
      "outputs": [],
      "source": [
        "def visualize_audio_signal(audio_wave):\n",
        "  signal, sr = audio_wave\n",
        "  mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=40)\n",
        "  librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
        "  plt.show()\n",
        "  mel_spec = librosa.feature.melspectrogram(y=signal)\n",
        "  mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "  librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel')\n",
        "  time = np.arange(0, len(signal)) / sr\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(time, signal)\n",
        "  ax.set(xlabel='Time(s)',ylabel='Amplitude')\n",
        "  plt.show()\n",
        "  df = pd.DataFrame(mfccs)\n",
        "  df\n",
        "\n",
        "visualize_audio_signal(audio_waves[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szDYL7of4Q5h"
      },
      "outputs": [],
      "source": [
        "del audio_waves\n",
        "del data\n",
        "del data2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtkdnfOXWvjMboP/A3XFyX",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}